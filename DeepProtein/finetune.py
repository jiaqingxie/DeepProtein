import os, sys
import torch
import torch.nn as nn

module_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if module_path not in sys.path:
    sys.path.append(module_path)
from DeepProtein.dataset import *

from transformers import (
    T5Config,
    T5EncoderModel,
    PreTrainedModel,
    T5Tokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)

from sklearn.model_selection import train_test_split
from datasets import Dataset

import numpy as np

# ==========================
# 1) åŠ è½½ 9 ä¸ª DataSet ç±»
# ==========================
# å•åºåˆ—å›å½’ (3)
train_fluo = FluorescenceDataset(os.getcwd() + '/DeepProtein/data', 'train')
train_beta = Beta_lactamase(os.getcwd() + '/DeepProtein/data', 'train')
train_stab = Stability(os.getcwd() + '/DeepProtein/data', 'train')

# å•åºåˆ—åˆ†ç±»
#   - "solu" -> äºŒåˆ†ç±»
#   - "sub"  -> ååˆ†ç±»
#   - "sub_bin" -> äºŒåˆ†ç±»
train_solu = Solubility(os.getcwd() + '/DeepProtein/data', 'train')
train_sub = Subcellular(os.getcwd() + '/DeepProtein/data', 'train')  # 10 ç±»
train_sub_bin = BinarySubcellular(os.getcwd() + '/DeepProtein/data', 'train')  # 2 ç±»

# åŒåºåˆ—äºŒåˆ†ç±»
train_yeast = Yeast_PPI(os.getcwd() + '/DeepProtein/data', 'train')  # 2 ç±»
train_human = HUMAN_PPI(os.getcwd() + '/DeepProtein/data', 'train')  # 2 ç±»

# åŒåºåˆ—å›å½’
train_aff = PPI_Affinity(os.getcwd() + '/DeepProtein/data', 'train')

# ===========================================
# 2) è‡ªå®šä¹‰ T5 Multi-Head æ¨¡å‹
# ===========================================
import torch
import torch.nn as nn
from transformers import PreTrainedModel, T5Config, T5EncoderModel


class T5MultiHeadModel(PreTrainedModel):
    config_class = T5Config

    def __init__(self, config, num_classes_binary=2, num_classes_10=10, d_model=None):
        super().__init__(config)

        # 1ï¸âƒ£ T5 Encoder å…±äº«ç‰¹å¾æå–
        self.encoder = T5EncoderModel(config)
        hidden_dim = d_model if d_model is not None else config.d_model

        # 2ï¸âƒ£ ä¸‰ä¸ªä»»åŠ¡å¤´
        self.classification_head_2 = nn.Linear(hidden_dim, num_classes_binary)
        self.classification_head_10 = nn.Linear(hidden_dim, num_classes_10)
        self.regression_head = nn.Linear(hidden_dim, 1)

        self.init_weights()

    def forward(self, input_ids=None, attention_mask=None, task_head_type=None, labels=None, **kwargs):
        """
        Args:
          - task_head_type: å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å¼ é‡, {"binary","tenclass","regression"}
          - labels: åˆ†ç±» => int, å›å½’ => float
        Returns:
          {
            "loss":   æ ‡é‡ or None,
            "logits_binary":     [N_b,2] or None,
            "logits_tenclass":   [N_t,10] or None,
            "logits_regression": [N_r] or None
          }
        """

        # 1ï¸âƒ£ T5 ç¼–ç å™¨
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = encoder_outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]

        # 2ï¸âƒ£ å–ç¬¬ä¸€ä¸ª token ä½œä¸º pooled representation
        pooled_output = hidden_states[:, 0, :]  # [batch_size, hidden_dim]

        # 3ï¸âƒ£ å¤„ç† `task_head_type`
        if isinstance(task_head_type, list):
            task_codes = torch.tensor(
                [0 if x == "binary" else 1 if x == "tenclass" else 2 for x in task_head_type],
                device=pooled_output.device
            )
        else:
            task_codes = task_head_type  # ç›´æ¥ä½¿ç”¨å¼ é‡

        binary_mask = (task_codes == 0)
        tenclass_mask = (task_codes == 1)
        regress_mask = (task_codes == 2)


        loss = None  # âœ… è¿™é‡Œç¡®ä¿ `loss` æ­£ç¡®åˆå§‹åŒ–

        # ========== å¤„ç†äºŒåˆ†ç±»ä»»åŠ¡ ==========
        if binary_mask.any():
            idx_b = binary_mask.nonzero(as_tuple=True)[0]
            pooled_b = pooled_output[idx_b]
            logits_binary = self.classification_head_2(pooled_b)  # [Nb,2]

            if labels is not None:
                b_labels = labels[idx_b].long()
                if len(b_labels) > 0:
                    loss_b = nn.CrossEntropyLoss()(logits_binary, b_labels)
                    loss = loss_b if loss is None else (loss + loss_b)

        # ========== å¤„ç†ååˆ†ç±»ä»»åŠ¡ ==========
        if tenclass_mask.any():
            idx_t = tenclass_mask.nonzero(as_tuple=True)[0]
            pooled_t = pooled_output[idx_t]
            logits_tenclass = self.classification_head_10(pooled_t)  # [Nt,10]

            if labels is not None:
                t_labels = labels[idx_t].long()
                if len(t_labels) > 0:
                    loss_t = nn.CrossEntropyLoss()(logits_tenclass, t_labels)
                    loss = loss_t if loss is None else (loss + loss_t)

        # ========== å¤„ç†å›å½’ä»»åŠ¡ ==========
        if regress_mask.any():
            idx_r = regress_mask.nonzero(as_tuple=True)[0]
            pooled_r = pooled_output[idx_r]
            logits_regression = self.regression_head(pooled_r).squeeze(-1)  # [Nr]

            if labels is not None:
                r_labels = labels[idx_r].float()

                # ğŸš€ ä¿®æ­£: é¿å… `NaN` loss
                if torch.isnan(r_labels).any():
                    print("âš ï¸ å‘ç° NaN labelï¼Œä¿®æ­£ä¸º 0!")
                    r_labels = torch.nan_to_num(r_labels, nan=0.0)

                if len(r_labels) > 0:
                    loss_r = nn.MSELoss()(logits_regression, r_labels)
                    loss = loss_r if loss is None else (loss + loss_r)

        # ğŸš€ ä¿®æ­£: é¢„é˜² `loss=0` å’Œ `grad_norm=NaN`
        if loss is None:
            loss = torch.tensor(0.0, device=pooled_output.device)

        return {
            "loss": loss,
        }


# ===========================================
# 3) æ„å»ºå¤šä»»åŠ¡ data_list
# ===========================================
def build_multi_head_data():
    data_list = []

    # å•åºåˆ—å›å½’
    def add_single_regression(dataset):
        for i in range(len(dataset)):
            seq, lab = dataset[i]
            data_list.append({
                "task_head_type": "regression",
                "sequence": seq,
                "label": lab
            })

    # å•åºåˆ—äºŒåˆ†ç±»
    def add_single_binary(dataset):
        for i in range(len(dataset)):
            seq, lab = dataset[i]
            data_list.append({
                "task_head_type": "binary",
                "sequence": seq,
                "label": lab
            })

    # å•åºåˆ—ååˆ†ç±»
    def add_single_tenclass(dataset):
        for i in range(len(dataset)):
            seq, lab = dataset[i]
            data_list.append({
                "task_head_type": "tenclass",
                "sequence": seq,
                "label": lab
            })

    # åŒåºåˆ—äºŒåˆ†ç±»
    def add_pair_binary(dataset):
        for i in range(len(dataset)):
            s1, s2, lab = dataset[i]
            data_list.append({
                "task_head_type": "binary",
                "sequence": s1 + " <sep> " + s2,
                "label": lab
            })

    # åŒåºåˆ—å›å½’
    def add_pair_regression(dataset):
        for i in range(len(dataset)):
            s1, s2, lab = dataset[i]
            data_list.append({
                "task_head_type": "regression",
                "sequence": s1 + " <sep> " + s2,
                "label": lab
            })

    # === å›å½’ (3) ===
    add_single_regression(train_fluo)
    add_single_regression(train_beta)
    add_single_regression(train_stab)

    # === å•åºåˆ—åˆ†ç±» ===
    # solu => 2-class
    add_single_binary(train_solu)
    # sub => 10-class
    add_single_tenclass(train_sub)
    # sub_bin => 2-class
    add_single_binary(train_sub_bin)

    # === åŒåºåˆ—äºŒåˆ†ç±» ===
    # add_pair_binary(train_yeast)
    # add_pair_binary(train_human)
    #
    # # === åŒåºåˆ—å›å½’ ===
    # add_pair_regression(train_aff)

    return data_list

# ===========================================
# 4) ä¸»å‡½æ•°
# ===========================================
def main():
    # 1) æ„å»ºå¤šä»»åŠ¡æ•°æ®
    all_data_list = build_multi_head_data()

    # 2) ç»Ÿä¸€ label çš„ç±»å‹ï¼Œä»¥å…æ··åˆæ•°ç»„å’Œæ ‡é‡
    for ex in all_data_list:
        lab = ex["label"]
        head_type = ex["task_head_type"]
        # å¦‚æœ label æ˜¯ numpy æ•°ç»„ï¼Œå°±å…ˆè½¬æˆæ ‡é‡æˆ– list
        if isinstance(lab, np.ndarray):
            if lab.size == 1:
                # å¦‚æœæ˜¯ä¸€ç»´å•å€¼æ•°ç»„ï¼Œç›´æ¥å–å‡ºæ¥
                lab = lab.item()  # å˜ä¸ºçº¯ Python æ ‡é‡
            else:
                # å¦‚æœæ˜¯å¤šç»´æ•°ç»„ï¼Œçœ‹éœ€è¦å¤šæ ‡ç­¾æˆ–åªå–ç¬¬ä¸€é¡¹ï¼Ÿ
                # æš‚æ—¶æ¼”ç¤ºï¼šç›´æ¥å– list => è¿™ä¼šå˜æˆ listï¼Œå¯èƒ½ä»ç„¶æ··ç±»å‹
                # å¦‚æœä½ ç¡®å®éœ€è¦å¤šç»´æ ‡ç­¾ï¼Œè¯·ä¿è¯æ‰€æœ‰æ ·æœ¬éƒ½ç»Ÿä¸€æˆ list
                lab = lab.tolist()

        # å¯¹åˆ†ç±»ä»»åŠ¡ => intï¼Œå¯¹å›å½’ä»»åŠ¡ => float
        if head_type in ["binary", "tenclass"]:
            # äºŒåˆ†ç±»æˆ–ååˆ†ç±» => int
            ex["label"] = int(lab)
        else:
            # å›å½’ => float
            ex["label"] = float(lab)

    # 3) è½¬ä¸º HF Dataset
    from collections import defaultdict
    def inspect_data_types(data_list):
        key_types = defaultdict(set)
        for i, example in enumerate(data_list):
            for k, v in example.items():
                key_types[k].add(type(v))
        for k, types_found in key_types.items():
            print(f"{k}: {types_found}")

    print("=== æ£€æŸ¥è½¬æ¢å data_list çš„ç±»å‹åˆ†å¸ƒ ===")
    inspect_data_types(all_data_list)

    full_dataset = Dataset.from_list(all_data_list)

    # 4) Train/Val åˆ‡åˆ†
    split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = split_dataset["train"]
    val_dataset = split_dataset["test"]




    # 5) Tokenizer
    MODEL_NAME = "Rostlab/prot_t5_xl_uniref50"
    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

    task_mapping = {"binary": 0, "tenclass": 1, "regression": 2}

    def tokenize_fn(examples):
        seqs = examples["sequence"]
        out = tokenizer(
            seqs,
            padding="max_length",
            truncation=True,
            max_length=512
        )

        # ç›´æ¥æ˜ å°„æˆæ•´æ•°
        out["task_head_type"] = [task_mapping[t] for t in examples["task_head_type"]]
        out["labels"] = examples["label"]
        return out

    train_dataset = train_dataset.map(tokenize_fn, batched=True)

    print(train_dataset["task_head_type"][:20])
    val_dataset = val_dataset.map(tokenize_fn, batched=True)

    data_collator = DataCollatorWithPadding(tokenizer)

    # 6) åˆå§‹åŒ–è‡ªå®šä¹‰ T5 å¤šå¤´æ¨¡å‹
    config = T5Config.from_pretrained(MODEL_NAME)
    model = T5MultiHeadModel.from_pretrained(
        MODEL_NAME,
        config=config,
        num_classes_binary=2,
        num_classes_10=10
    )

    # 7) è‡ªå®šä¹‰ Trainer
    class MyTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)

        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
            has_labels = "labels" in inputs
            inputs = self._prepare_inputs(inputs)
            with torch.no_grad():
                outputs = model(**inputs)
            loss = outputs["loss"] if has_labels else None
            if prediction_loss_only:
                return (loss, None, None)

            # outputs: {'loss','logits_binary','logits_tenclass','logits_regression'}
            logits_dict = {
                "binary": outputs["logits_binary"],
                "tenclass": outputs["logits_tenclass"],
                "regression": outputs["logits_regression"]
            }
            labels = inputs["labels"] if has_labels else None
            task_types = inputs["task_head_type"]

            return (loss, (logits_dict, task_types), labels)

    def compute_metrics(eval_pred):
        (logits_dict, task_types) = eval_pred[0]
        labels = eval_pred[1]

        import numpy as np
        binary_preds = []
        binary_labels = []
        ten_preds = []
        ten_labels = []
        reg_preds = []
        reg_labels = []

        logits_bin = logits_dict["binary"]
        logits_10 = logits_dict["tenclass"]
        logits_reg = logits_dict["regression"]

        # è½¬åˆ° CPU
        logits_bin = logits_bin.detach().cpu().numpy() if logits_bin is not None else None
        logits_10 = logits_10.detach().cpu().numpy() if logits_10 is not None else None
        logits_reg = logits_reg.detach().cpu().numpy() if logits_reg is not None else None

        if torch.is_tensor(labels):
            lbls = labels.detach().cpu().numpy()
        else:
            lbls = np.array(labels)

        # å¦‚æœ task_types ä¹Ÿæ˜¯å¼ é‡ï¼Œéœ€è¦æ˜ å°„æˆå­—ç¬¦ä¸²ï¼›å‡è®¾ç›®å‰æ˜¯ ["binary","tenclass","regression",...]
        # è¿™é‡Œç›´æ¥å½“æˆ list å¤„ç†
        n = len(task_types)
        idx_bin = 0
        idx_10 = 0
        idx_reg = 0

        for i in range(n):
            ttype = task_types[i]
            if ttype == "binary":
                pred_probs = logits_bin[idx_bin]  # [2]
                pred_class = np.argmax(pred_probs)
                true_label = lbls[i]
                binary_preds.append(pred_class)
                binary_labels.append(true_label)
                idx_bin += 1
            elif ttype == "tenclass":
                pred_probs = logits_10[idx_10]  # [10]
                pred_class = np.argmax(pred_probs)
                true_label = lbls[i]
                ten_preds.append(pred_class)
                ten_labels.append(true_label)
                idx_10 += 1
            else:  # "regression"
                pred_val = logits_reg[idx_reg]  # scalar
                true_val = lbls[i]
                reg_preds.append(pred_val)
                reg_labels.append(true_val)
                idx_reg += 1

        metrics = {}
        # Binary å‡†ç¡®ç‡
        if len(binary_preds) > 0:
            bin_acc = (np.array(binary_preds) == np.array(binary_labels)).mean()
            metrics["binary_acc"] = bin_acc
        # Ten-class å‡†ç¡®ç‡
        if len(ten_preds) > 0:
            ten_acc = (np.array(ten_preds) == np.array(ten_labels)).mean()
            metrics["tenclass_acc"] = ten_acc
        # å›å½’ MSE
        if len(reg_preds) > 0:
            reg_mse = ((np.array(reg_preds) - np.array(reg_labels)) ** 2).mean()
            metrics["regression_mse"] = reg_mse

        return metrics

    training_args = TrainingArguments(
        output_dir="/cluster/scratch/jiaxie/models/DeepProtT5",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=3,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        learning_rate=1e-5,
        weight_decay=0.01,
        logging_steps=1,
        gradient_accumulation_steps=2,
        max_grad_norm=1.0,
        load_best_model_at_end=True,
        fp16=True
    )

    trainer = MyTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model("/cluster/scratch/jiaxie/models/DeepProtT5")
    tokenizer.save_pretrained("/cluster/scratch/jiaxie/models/DeepProtT5")
    print("Done. Model + tokenizer saved.")


if __name__ == "__main__":
    main()
